[directories]
#directory where the training data will be retrieved
train_data = /esat/spchtemp/scratch/moritz/dataSets/timit/s5/data/train
#directory where the testing data will be retrieved
test_data = /esat/spchtemp/scratch/moritz/dataSets/timit/s5/data/test
#directory where the training features will be stored and retrieved
train_features = /esat/spchtemp/scratch/moritz/beam_tfkaldi/TIMIT/features/train
#directory where the testing features will be stored and retrieved
test_features = /esat/spchtemp/scratch/moritz/beam_tfkaldi/TIMIT/features/test
#directory where the data from this experiment will be stored (logs,models,...)
expdir = /esat/spchtemp/scratch/moritz/beam_tfkaldi/TIMIT/expdir

[dnn-features]
#name of the features. GMM features => same name.
name = 40fbankdd
#feature type options: mfcc, fbank and ssc
type = fbank
#the dynamic information that is added to the features,
#   i.e. nodelta, delta and ddelta
dynamic = ddelta
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 40
#number of cepstral features
numcep = 12
#mfcc option: cepstral lifter (used to scale the mfccs)
ceplifter = 22
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = True
#snip the edges for sliding window
snip_edges = True


[nnet] #name of the neural net
name = dropout_test_no_dropout_e10
#--------------------Listener:------------------------------------------------#
# number of neurons in the hidden layers
num_units = 128
# number of pyramidal layers in the listener
num_layers = 1
# listener output dimension.
output_dim = None
#-----------Attend and spell:-------------------------------------------------#
#set the decoder state size
state_size = 256
#set the network sizes of the state and feature nets.
net_size = 128
#choose the number of hidden layers for the feedforward nets.
n_hidden = 1
# set the probability of picking the network output instead of the groundtruth
# during training.
net_out_prob = 0.8
#input noise standard deviation
input_noise_std = 0.0
#l2 normalization weight
l2_cost_weight = 0.0 
#dropout parameters
input_keep_prob = 1.0
hidden_keep_prob = 1.0

#the decoding beam width
beam_width = 39

#starting step, set to 'final' to skip nnet training
starting_step = 0
#number of passes over the entire database
num_epochs = 10
#initial learning rate of the neural net
initial_learning_rate = 1e-3
#exponential weight decay parameter
learning_rate_decay = 1
#l2 normalization weight
l2_cost_weight = 0.0
#size of the batch (#utterances)
batch_size = 16
#to limit memory ussage (specifically for GPU) the batch can be devided into
#even smaller batches. The gradient will be calculated by averaging the
#gradients of all these mini-batches. This value is the size of these
#mini-batches in number of utterances. For optimal speed this value should be
#set as high as possible without exeeding the memory. To use the entire batch
#set to -1
numutterances_per_minibatch = 16
#size of the validation set, set to 0 if you don't want to use one
valid_batches = 4
#frequency of evaluating the validation set. CATION: If the valid_frequency is
#more than 5 times larger than the checkpoint frequency the validated model
#will be deleted by the saver
valid_frequency = 500
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = False
#number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 3
#how many steps are taken between two checkpoints
check_freq = 500
#you can visualise the progress of the neural net with tensorboard
visualise = True

